apiVersion: distributed-compute.dominodatalab.com/v1alpha1
kind: SparkCluster
metadata:
  name: example
spec:
  ## global configurations

  # image used by head and worker pods
  image:
    registry: ""
    repository: bitnami/spark
    #    tag: 3.0.2-debian-10-r0
    tag: 3.1.1
    pullPolicy: IfNotPresent

  # dockerconfigjson used to pull images from private registries
  # imagePullSecrets:
  # - name: docker-registry-credentials

  # enable autoscaling (worker cpu requests also required)
  # autoscaling:
  #   minReplicas: 1
  #   maxReplicas: 5
  #   averageUtilization: 50
  #   scaleDownStabilizationWindowSeconds: 60

  # port used to connect clients to spark
  # clientServerPort: 10001

  # enable dashboard ui and set port
  dashboardPort: 8080
  enableDashboard: true

  # internal port overrides
  clusterPort: 7077

  # driver service config
  sparkClusterDriver:
    sparkClusterName: example-spark
    executionName: run-example-spark
    driverPortName: spark-driver-port
    driverPort: 4040
    driverUIPortName: spark-ui-port
    driverUIPort: 4041
    driverBlockManagerPortName: spark-block-manager-port
    driverBlockManagerPort: 4042

  # control the creation of network policies
  networkPolicy:
    enabled: true
    clusterLabels:
      np: cluster
    dashboardLabels:
      np: dash
    clientServerLabels:
      np: client

  # bind cluster to existing psp
  # podSecurityPolicy: privileged

  # specify the security context used by spark pods
  # podSecurityContext:
  #   runAsUser: 0

  # specify the service account used to run spark pods
  # serviceAccountName: my-service-account

  # add extra env vars to every spark pod container.
  # envVars:
  #  - name: SPARK_WORKER_CORES
  #    value: 2
  # Here are a number of other potentially useful examples
  #  - name: SPARK_WORKER_MEMORY
  #  - name: SPARK_WORKER_PORT
  #  - name: SPARK_WORKER_WEBUI_PORT
  #  - name: SPARK_WORKER_DIR
  #  - name: SPARK_DAEMON_JAVA_OPTS
  #  - name: SPARK_MASTER_URL
  #  - name: SPARK_WORKER_OPTS
  #  - name: SPARK_RPC_AUTHENTICATION_ENABLED
  #  - name: SPARK_RPC_AUTHENTICATION_SECRET
  #  - name: SPARK_RPC_ENCRYPTION_ENABLED
  #  - name: SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED
  #  - name: SPARK_SSL_ENABLED
  #  - name: SPARK_SSL_KEY_PASSWORD
  #  - name: SPARK_SSL_KEYSTORE_PASSWORD
  #  - name: SPARK_SSL_TRUSTSTORE_PASSWORD
  #  - name: SPARK_SSL_NEED_CLIENT_AUTH
  #  - name: SPARK_SSL_PROTOCOL
  #  - name: SPARK_MASTER_PORT
  #  - name: SPARK_MASTER_WEBUI_PORT
  #  - name: SPARK_MASTER_OPTS

  #  autoscaling:
  #    averageCPUUtilization: 5
  #    maxReplicas: 3
  #    minReplicas: 1
  #    scaleDownStabilizationWindowSeconds: 10

  # override istio's peer authentication policy
  # istioMutualTLSMode: PERMISSIVE

  ## head configuration parameters

  head:
    #   labels:
    #     environment: production

    #   annotations:
    #     non-identifying-metadata: "true"

    #   nodeSelector:
    #     disktype: ssd

    #   affinity:
    #     nodeAffinity:
    #       preferredDuringSchedulingIgnoredDuringExecution:
    #       - weight: 1
    #         preference:
    #           matchExpressions:
    #           - key: another-node-label-key
    #             operator: In
    #             values:
    #             - another-node-label-value

    #   tolerations:
    #   - key: example-key
    #     operator: Equal
    #     value: example-value
    #     effect: NoSchedule

    #   initContainers:
    #   - name: busybox
    #     command: custom_setup.sh

    #   volumes:
    #   - name: example-vol
    #     emptyDir: {}

    #   volumeMounts:
    #   - name: example-vol
    #     mountPath: /mnt/more/space

    resources:
      requests:
        cpu: 100m
        memory: 250Mi
      limits:
        cpu: 500m
        memory: 512Mi
  #    frameworkConfig:
  #      path: /opt/bitnami/spark/conf/spark-defaults.conf
  #      configs:
  #        spark.driver.host: "run-dc-607624b1336b20009d4d081c-driver.audidco273377-compute.svc.cluster.local"
  #        spark.executor.cores: "4"
  #        spark.executor.instances: "1"
  #        spark.executor.memory: 15360
  #        spark.ui.proxyBase: .
  #        spark.ui.reverseProxy: "true"
  #        spark.ui.reverseProxyUrl: "/integration-test/quick-start/workspace/607624b1336b20009d4d081c/spark/ui/"
  #        spark.ui.reverseProxy: "true"
  #        spark.ui.reverseProxyUrl: "/master/proxy/base"
  #        spark.ui.proxyBase: "/master/proxy/url"
  #    keyTabConfig:
  #      path: /tmp/keytab.conf
  #      configs: dGVzdHZhbHVlCg==


  ## worker configuration parameters

  worker:
    # total number of worker pods
    replicas: 1

    # labels:
    #   environment: production

    # annotations:
    #   non-identifying-metadata: "true"

    # nodeSelector:
    #   disktype: ssd

    # affinity:
    #   nodeAffinity:
    #     preferredDuringSchedulingIgnoredDuringExecution:
    #     - weight: 1
    #       preference:
    #         matchExpressions:
    #         - key: another-node-label-key
    #           operator: In
    #           values:
    #           - another-node-label-value

    # tolerations:
    # - key: example-key
    #   operator: Equal
    #   value: example-value
    #   effect: NoSchedule

    # initContainers:
    #   - name: busybox
    #     command: custom_setup.sh

    # volumes:
    #   - name: example-vol
    #     emptyDir: {}

    # volumeMounts:
    #   - name: example-vol
    #     mountPath: /mnt/more/space
    #    volumes:
    #    - name: spark-data
    #      persistentVolumeClaim:
    #        claimName: spark-data-pvc
    #    volumeMounts:
    #    - name: spark-data
    #      mountPath: /data

    resources:
      requests:
        # NOTE: this must be set when attempting to use HPAs
        cpu: 100m
        memory: 250Mi
      limits:
        cpu: 500m
        memory: 512Mi

#    frameworkConfig:
#      path: /opt/bitnami/spark/conf/spark-defaults.conf
#      configs:
#        spark.driver.host: "run-dc-607624b1336b20009d4d081c-driver.audidco273377-compute.svc.cluster.local"
#        spark.executor.cores: "4"
#        spark.executor.instances: "1"
#        spark.executor.memory: 15360m
#        spark.ui.proxyBase: .
#        spark.ui.reverseProxy: "true"
#        spark.ui.reverseProxyUrl: "/integration-test/quick-start/workspace/607624b1336b20009d4d081c/spark/ui/KK"
#        spark.ui.reverseProxy: "true"
#        spark.ui.reverseProxyUrl: "/worker/proxy/url"
#        spark.ui.proxyBase: "/master/proxy/base"

#    additionalStorage:
#    - accessModes:
#      - ReadWriteMany
#      size: 1Gi
#      storageClass: nfs
#      name: spark-worker-pvc
